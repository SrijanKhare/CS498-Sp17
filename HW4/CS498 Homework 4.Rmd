---
title: "CS498 Homework 4"
author: "Xinchen Pan, Fangxiaozhi Yu, Jiefei Shi"
date: "March 2, 2017"
output: 
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
---

```{r setup, global options,include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align='center')
```

##7.9

The dataset we used has 21 observations and two variables. \texttt{Sulfate} is our response variable and \texttt{hours} is the predictor. We built a simple linear regression model of the log of the \texttt{Sulfate} against the log of \texttt{Hours}.

```{r,echo=FALSE}
dt <- read.table("brunhild.txt", sep = "\t", header = T)
mod1 <- lm(log(Sulfate) ~ log(Hours), data = dt)
mod1
```

**(a)**

Below is the plot showing the data points and the regression line in log-log coordinates.


```{r}
knitr::include_graphics("Rplot.png")
```


**(b)**

To show the data points and regression curve in the original coordinates, I did the following transformation.
\[
\begin{aligned}
\widehat{\text{log(Sulfate)}}&=-0.247*\text{log(Hours)}+2.766\\
\text{exp}(\widehat{\text{log(Sulfate)}}&=\text{exp}(-0.247*\text{log(Hours)}+2.766)\\
\widehat{\text{Sulfate}}&=\text{Hours}^{-0.247}*\text{exp}(2.766)
\end{aligned}
\]

Below is the plot showing the data points and the regression curve in original coordinates.


```{r}
knitr::include_graphics("Rplot1.png")
```

**(c)**
Below is the residual against the fitted values plot in log-log and in original coordinates.

```{r}
knitr::include_graphics("Rplot2.png")
```

**(d)**

From the first plot, we can see that the data points are almost all on the regression line. It means that the regression model does a good job in predicting. Checking the second plot, we find that in original coordinates, the data points are also almost all on the regression curve.  However, by checking both plots of the **Residual vs Fitted** plot, we can observe a very obvious **pattern** or **trend**. As the fitted value gets larger, the residual will be larger. Thus the model will not do a good job in predicting large values. Thus, our model might **not** be a very good model.

##7.10

For this problem, the dataset we used has 22 observations and 11 variables. We were predicting \texttt{Mass} using all other predictors.

```{r}
dt2 <- read.table("physical.txt", sep = "\t", header = T)
mod3 <- lm(Mass ~., data = dt2)
mod3
```

**(a)**

Below is the residual against the fitted values plot for the regression model we made.

```{r}
knitr::include_graphics("Rplot3.png")
```

**(b)**

We firstly made the model of the cube root of mass against other parameters.
```{r}
mod4 <- lm(Mass^(1/3) ~., data = dt2)
mod4
```


To get the residuals in the original coordinates, we did something like this. We cubed the fitted values first, then we used the original values to minus these values.

Below is the residuals again the fitteed values plot in both cube root coordinates and original coordinates.


```{r}
knitr::include_graphics("Rplot4.png")
```

**(c)**

Both regression models are good from the **Residual vs Fitted** plots. We can see that the trends in both plots are roughly flat with equal vertical spread. The mean of the residual seems to be 0 in both plots. Thus we believe both models are good and can make good predictions.

##7.11

```{r}
dt3 <- read.table("abalone.data", sep = ",")
Names <- c("Sex", "Length", "Diameter", "Height", "Whole_weight", "Sucked_weight",
           "Viscera_weight", "Shell_weight", "rings")

colnames(dt3) <- Names
```

The dataset we used for this problem has 4177 observations and 9 variables. What we wanted to predict is the age of abalone. It was represented as number of \texttt{rings} in the data. 

**(a)**

Age vs other measures, ignoring gender

```{r}
mod5 <- lm(rings ~ .-Sex, data = dt3)
mod5
```

```{r,fig.align='center'}
knitr::include_graphics("Rplot5.png")
```

**(b)**

Age vs other measures, including gender

```{r}
mod6 <- lm(rings ~., data = dt3)
mod6
```

```{r}
knitr::include_graphics("Rplot6.png")
```

**(c)**

log(Age) vs other measures, ignoring gender

```{r}
mod7 <- lm(log(rings) ~. - Sex, data = dt3)
mod7
```

```{r}
knitr::include_graphics("Rplot7.png")
```

**(d)**
log(Age) vs other measures, including gender

```{r}
mod8 <- lm(log(rings) ~., data = dt3)
mod8
```

```{r}
knitr::include_graphics("Rplot8.png")
```


**(e)**

From the four plots above, we can see that basically gender does **not** have an effect in predicting the ages. The reason is that the plots do not change much after ignoring the \texttt{gender} predictor. However, transformation of the \texttt{rings} make a difference. After taking the log for the response variable, the **Residual vs Fitted** plots have a flat trend and vertical evenly spread. Before log transformation, the pattern looks like a funnel towards right. That is saying for larger fitted values, the residuals will be larger too.

Thus we would choose the regression model from **c** or **d**. We believe they will do a good job in prediction.

**(f)**

We made a total of four models for questiosn (a)-(d). For this question, we tried two regularization methods, \texttt{ridge} and \texttt{lasso}, to check if we could improve the regression models. We made 8 models in total and produced 8 plots.

In the \texttt{glmnet} package, the default regularization method is ridge. The parameter is `alpha = 0`. Setting `alpha = 1`, the method changes to lasso. We used
\texttt{cv.glmnet} to do cross validations. The default fold number is 10. The regularization value \(\lambda\) is determined automatically by the function. It tested 100(default) \(lamada\) values and choose the one with lowest cross-validation error. 


```{r}
a <- 5.404472
b<-  5.052858
c <- 5.316659
d <- 4.922656
e <- 0.04631969
f <- 0.04338792
g <- 0.04450456
h <- 0.04218863
mse <- 4.909237
mse1 <- 4.802664
mse2 <- 0.04230998
mse3 <- 0.04092327
```

```{r}
Rownames <- c("Age, ignoring sex", "Age, including sex", "log(Age), ignoring sex", "log(Age), including sex")
dt4 <- data.frame("Models" = Rownames, "Ridge" = c(a,c,e,g), "Lasso" = c(b,d,f,h), "No Regularizer" = c(mse,mse1,mse2,mse3),check.names = FALSE)
```

```{r}
library(knitr)
```


From the table below, we can see that regularzation method **does not** improve
the regressions. Lasso and Ridge are often used for the purpose of control over-fitting and deal with multicollinearty among the predictors. In our case, there are basically no collinearity among predictors. As we can see from the plots that none of the variables was dropped for lasso. Thus regularation methods are not helpful here. We also noticed Lasso method behaves better than Ridge regression. **In summation of above, the models without regularizers are better**.


```{r}
kable(dt4,caption = "MSE Table")
```

\newpage

The following plots are log(Lambda) vs Mean-Squared Error plots

Upper left: Age vs other variables, ignoring gender (Ridge regression)   
Upper right: Age vs other variables, ignoring gender(Lasso regression)  
Bottom left: Age vs other variables, including gender (Ridge regression)  
Bottom right: Age vs other variables, including gender (Lasso regression)  

```{r}
knitr::include_graphics("Rplot01.png")
```

\newpage
The following plots are log(Lambda) vs Mean-Squared Error plots

Upper left: Log(Age) vs other variables, ignoring gender (Ridge regression)  
Upper right: Log(Age) vs other variables, ignoring gender(Lasso regression)  
Bottom left: Log(Age) vs other variables, including gender (Ridge regression)  
Bottom right: Log(Age) vs other variables, including gender (Lasso regression)


```{r}
knitr::include_graphics("Rplot02.png")
```






